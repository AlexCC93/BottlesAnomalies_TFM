{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uci_digits_auto_anomaly.py\n",
    "\n",
    "# autoencoder reconstruction error anomaly detection\n",
    "# uses an encoder-decoder architecture\n",
    "# PyTorch 1.8.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
    "# Windows 10 \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "\n",
    "device = T.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "class UCI_Digits_Dataset(T.utils.data.Dataset):\n",
    "  # 8,12,0,16, . . 15,7\n",
    "  # 64 pixel values [0-16], digit [0-9]\n",
    "\n",
    "  def __init__(self, src_file, n_rows=None):\n",
    "    all_xy = np.loadtxt(src_file, max_rows=n_rows,\n",
    "      usecols=range(0,65), delimiter=\",\", comments=\"#\",\n",
    "      dtype=np.float32)\n",
    "    self.xy_data = T.tensor(all_xy, dtype=T.float32).to(device) \n",
    "    self.xy_data[:, 0:64] /= 16.0   # normalize pixels\n",
    "    self.xy_data[:, 64] /= 9.0      # normalize digit/label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.xy_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    xy = self.xy_data[idx]\n",
    "    return xy\n",
    "\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "class Autoencoder(T.nn.Module):  # 65-32-8-32-65\n",
    "  def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.fc1 = T.nn.Linear(65, 32)\n",
    "    self.fc2 = T.nn.Linear(32, 8)\n",
    "    self.fc3 = T.nn.Linear(8, 32)\n",
    "    self.fc4 = T.nn.Linear(32, 65)\n",
    "\n",
    "  def encode(self, x):  # 65-32-8\n",
    "    z = T.tanh(self.fc1(x))\n",
    "    z = T.tanh(self.fc2(z))  # latent in [-1,+1]\n",
    "    return z  \n",
    "\n",
    "  def decode(self, x):  # 8-32-65\n",
    "    z = T.tanh(self.fc3(x))\n",
    "    z = T.sigmoid(self.fc4(z))  # [0.0, 1.0]\n",
    "    return z\n",
    "    \n",
    "  def forward(self, x):\n",
    "    z = self.encode(x) \n",
    "    z = self.decode(z) \n",
    "    return z  # in [0.0, 1.0]\n",
    "\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "\n",
    "def display_digit(ds, idx, save=False):\n",
    "  # ds is a PyTorch Dataset\n",
    "  line = ds[idx]  # tensor\n",
    "  pixels = np.array(line[0:64])  # numpy row of pixels\n",
    "  label = np.int(line[64] * 9.0)  # denormalize; like '5'\n",
    "  print(\"\\ndigit = \", str(label), \"\\n\")\n",
    "\n",
    "  pixels = pixels.reshape((8,8))\n",
    "  for i in range(8):\n",
    "    for j in range(8):\n",
    "      pxl = pixels[i,j]  # or [i][j] syntax\n",
    "      pxl = np.int(pxl * 16.0)  # denormalize\n",
    "      print(\"%.2X\" % pxl, end=\"\")\n",
    "      print(\" \", end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "  plt.imshow(pixels, cmap=plt.get_cmap('gray_r'))\n",
    "  if save == True:\n",
    "    plt.savefig(\".\\\\idx_\" + str(idx) + \"_digit_\" + \\\n",
    "    str(label) + \".jpg\", bbox_inches='tight')\n",
    "  plt.show() \n",
    "  plt.close() \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_digits(ds, idxs, save=False):\n",
    "  # idxs is a list of indices\n",
    "  for idx in idxs:\n",
    "    display_digit(ds, idx, save)\n",
    "\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(ae, ds, bs, me, le, lr):\n",
    "  # autoencoder, dataset, batch_size, max_epochs,\n",
    "  # log_every, learn_rate\n",
    "  # assumes ae.train() has been set\n",
    "  data_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  opt = T.optim.SGD(ae.parameters(), lr=lr)\n",
    "  print(\"\\nStarting training\")\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0\n",
    "    for (batch_idx, batch) in enumerate(data_ldr):\n",
    "      X = batch  # inputs\n",
    "      Y = batch  # targets (same as inputs)\n",
    "\n",
    "      opt.zero_grad()                # prepare gradients\n",
    "      oupt = ae(X)                   # compute output/target\n",
    "      loss_val = loss_func(oupt, Y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate for display\n",
    "      loss_val.backward()            # compute gradients\n",
    "      opt.step()                     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "  print(\"Done \")\n",
    "\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_err_list(model, ds):\n",
    "  # assumes model.eval()\n",
    "  result_lst = []\n",
    "  n_features = len(ds[0])  # 65\n",
    "  for i in range(len(ds)):\n",
    "    X = ds[i]\n",
    "    with T.no_grad():\n",
    "      Y = model(X)  # should be same as X\n",
    "    err = T.sum((X-Y)*(X-Y)).item()  # SSE all features\n",
    "    err = err / n_features           # sort of norm'ed SSE \n",
    "    result_lst.append( (i,err) )     # idx of data item, err\n",
    "  return result_lst \n",
    "\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin UCI Digits autoencoder anomaly demo \n",
      "\n",
      "Loading data as normalized tensors \n"
     ]
    }
   ],
   "source": [
    "# 0. get started\n",
    "print(\"\\nBegin UCI Digits autoencoder anomaly demo \")\n",
    "T.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# 1. create Dataset object\n",
    "print(\"\\nLoading data as normalized tensors \")\n",
    "fn = \".\\\\Data\\\\optdigits_train_3823.txt\"\n",
    "data_ds = UCI_Digits_Dataset(fn)  # all rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "3823\n"
     ]
    }
   ],
   "source": [
    "print(type(data_ds.__getitem__(1125)))\n",
    "print(len(data_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ae, ds, bs, me, le, lr):\n",
    "  # autoencoder, dataset, batch_size, max_epochs,\n",
    "  # log_every, learn_rate\n",
    "  # assumes ae.train() has been set\n",
    "  data_ldr = T.utils.data.DataLoader(ds, batch_size=bs,\n",
    "    shuffle=True)\n",
    "  loss_func = T.nn.MSELoss()\n",
    "  opt = T.optim.SGD(ae.parameters(), lr=lr)\n",
    "  print(\"\\nStarting training\")\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0\n",
    "    for (batch_idx, batch) in enumerate(data_ldr):\n",
    "      X = batch  # inputs\n",
    "      Y = batch  # targets (same as inputs)\n",
    "\n",
    "      opt.zero_grad()                # prepare gradients\n",
    "      oupt = ae(X)                   # compute output/target\n",
    "      loss_val = loss_func(oupt, Y)  # a tensor\n",
    "      epoch_loss += loss_val.item()  # accumulate for display\n",
    "      loss_val.backward()            # compute gradients\n",
    "      opt.step()                     # update weights\n",
    "\n",
    "    if epoch % le == 0:\n",
    "      print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "  print(\"Done \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a 65-32-8-32-65 autoencoder \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (fc1): Linear(in_features=65, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=32, bias=True)\n",
       "  (fc4): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 2. create autoencoder net\n",
    "print(\"\\nCreating a 65-32-8-32-65 autoencoder \")\n",
    "autoenc = Autoencoder().to(device)\n",
    "autoenc.train()   # set mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_size = 10\n",
    "\n",
    "data_ldr = T.utils.data.DataLoader(data_ds, batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    }
   ],
   "source": [
    "print(len(data_ldr)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is 383 because there were in total 3823 elements in the dataset (see \"len(data_ds)\"). And as this variable (data_ldr) is storing batches of data; each one of them of length 10, it only has the size of 383. Meaning that the last batch might contain only 3 elements instead of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bat_size = 10\n",
    "# lrn_rate = 0.005\n",
    "# max_epochs = 100\n",
    "# log_interval = 10\n",
    "\n",
    "\n",
    "# data_ldr = T.utils.data.DataLoader(data_ds, batch_size=bat_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loss_func = T.nn.MSELoss()\n",
    "# opt = T.optim.SGD(autoenc.parameters(), lr=lrn_rate)\n",
    "# print(\"\\nStarting training\")\n",
    "# for epoch in range(0, max_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     for (batch_idx, batch) in enumerate(data_ldr):\n",
    "#         X = batch  # inputs\n",
    "#         Y = batch  # targets (same as inputs)\n",
    "\n",
    "#         opt.zero_grad()                # prepare gradients\n",
    "#         oupt = autoenc(X)                   # compute output/target\n",
    "#         loss_val = loss_func(oupt, Y)  # a tensor\n",
    "#         epoch_loss += loss_val.item()  # accumulate for display\n",
    "#         loss_val.backward()            # compute gradients\n",
    "#         opt.step()                     # update weights\n",
    "\n",
    "#     if epoch % log_interval == 0:\n",
    "#         print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "# print(\"Done \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bat_size =  10 \n",
      "max epochs = 100\n",
      "loss = MSELoss\n",
      "optimizer = SGD\n",
      "lrn_rate = 0.005 \n",
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 69.7865\n",
      "epoch =   10   loss = 59.2447\n",
      "epoch =   20   loss = 45.8365\n",
      "epoch =   30   loss = 36.6743\n",
      "epoch =   40   loss = 32.6010\n",
      "epoch =   50   loss = 30.8375\n",
      "epoch =   60   loss = 29.9937\n",
      "epoch =   70   loss = 29.5430\n",
      "epoch =   80   loss = 29.2588\n",
      "epoch =   90   loss = 29.0767\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "# 3. train autoencoder model\n",
    "bat_size = 10\n",
    "max_epochs = 100\n",
    "log_interval = 10\n",
    "lrn_rate = 0.005\n",
    "\n",
    "print(\"\\nbat_size = %3d \" % bat_size)\n",
    "print(\"max epochs = \" + str(max_epochs))\n",
    "print(\"loss = MSELoss\")\n",
    "print(\"optimizer = SGD\")\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "train(autoenc, data_ds, bat_size, max_epochs, \\\n",
    "log_interval, lrn_rate) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../../BottlesAnomalies_TFM/models/UCI_digitsModel1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.save(autoenc.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(110, 225, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(225, 450, kernel_size=(8, 8), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(450, 225, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(225, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(110, 3, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For loading the model \n",
    "autoenc.load_state_dict(T.load(filepath))\n",
    "autoenc.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we will see the tensor max and min values of both, the input data and the reconstructed one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data treated through batches\n",
      "The len of each batch is:  10\n",
      "The images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the input is:  tensor(0.) tensor(1.)\n",
      "\n",
      "\n",
      "The reconstructed data treated through batches\n",
      "The recontructed images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the reconstruction is:  tensor(0.0677, grad_fn=<MinBackward1>) tensor(0.7495, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"The input data treated through batches\")\n",
    "images = next(iter(data_ldr))\n",
    "print(\"The len of each batch is: \", len(images))\n",
    "print(\"The images are stored in a variable of type: \", type(images))\n",
    "print(\"The min and max tensor values of the input is: \", T.min(images), T.max(images))\n",
    "print(\"\\n\")\n",
    "print(\"The reconstructed data treated through batches\")\n",
    "recon_output = autoenc(images)\n",
    "print(\"The recontructed images are stored in a variable of type: \", type(recon_output))\n",
    "print(\"The min and max tensor values of the reconstruction is: \", T.min(recon_output), T.max(recon_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor dimensions of both, the input and reconstruction data don't match. Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data treated through batches\n",
      "The len of each batch is:  10\n",
      "The images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the input is:  tensor(0.) tensor(1.)\n",
      "\n",
      "\n",
      "The reconstructed data treated through batches\n",
      "The recontructed images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the reconstruction is:  tensor(0.0665) tensor(0.7512)\n"
     ]
    }
   ],
   "source": [
    "print(\"The input data treated through batches\")\n",
    "images = next(iter(data_ldr))\n",
    "print(\"The len of each batch is: \", len(images))\n",
    "print(\"The images are stored in a variable of type: \", type(images))\n",
    "print(\"The min and max tensor values of the input is: \", T.min(images), T.max(images))\n",
    "print(\"\\n\")\n",
    "print(\"The reconstructed data treated through batches\")\n",
    "with T.no_grad():       # Because gradients will not be computed\n",
    "    recon_output = autoenc(images)\n",
    "print(\"The recontructed images are stored in a variable of type: \", type(recon_output))\n",
    "print(\"The min and max tensor values of the reconstruction is: \", T.min(recon_output), T.max(recon_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dimensions match. Using the \"T.no_grad()\" is imporant for this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this example uses not all the batch for computing the error of the reconstructed data but only one sample of the input data. See the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data considering only one sample. Not a batch of samples\n",
      "The images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the input is:  tensor(0.) tensor(1.)\n",
      "\n",
      "\n",
      "The reconstructed data onsidering only one sample. Not a batch of samples\n",
      "The recontructed images are stored in a variable of type:  <class 'torch.Tensor'>\n",
      "The min and max tensor values of the reconstruction is:  tensor(0.0707) tensor(0.7461)\n"
     ]
    }
   ],
   "source": [
    "print(\"The input data considering only one sample. Not a batch of samples\")\n",
    "X = data_ds[0]\n",
    "print(\"The images are stored in a variable of type: \", type(X))\n",
    "print(\"The min and max tensor values of the input is: \", T.min(X), T.max(X))\n",
    "print(\"\\n\")\n",
    "print(\"The reconstructed data onsidering only one sample. Not a batch of samples\")\n",
    "with T.no_grad():       # Because gradients will not be computed\n",
    "    Y = autoenc(X)\n",
    "print(\"The recontructed images are stored in a variable of type: \", type(Y))\n",
    "print(\"The min and max tensor values of the reconstruction is: \", T.min(Y), T.max(Y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that the tnesor max and min values of the input and reconstruction data are not the same, but somehow close to each other. This is expected as the reconstruction is not 100% the same as the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result_lst = []\n",
    "# n_features = len(data_ds[0])  # 65\n",
    "# for i in range(len(data_ds)):\n",
    "#     X = data_ds[i]\n",
    "#     with T.no_grad():\n",
    "#         Y = autoenc(X)  # should be same as X\n",
    "#     err = T.sum((X-Y)*(X-Y)).item()  # SSE all features\n",
    "#     err = err / n_features           # sort of norm'ed SSE \n",
    "# result_lst.append( (i,err) )     # idx of data item, err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing reconstruction errors \n"
     ]
    }
   ],
   "source": [
    "# 4. compute and store reconstruction errors\n",
    "print(\"\\nComputing reconstruction errors \")\n",
    "autoenc.eval()  # set mode\n",
    "err_list = make_err_list(autoenc, data_ds)\n",
    "err_list.sort(key=lambda x: x[1], \\\n",
    "reverse=True)  # high error to low\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest reconstruction item / error: \n",
      " [ 486]  0.1352\n",
      "\n",
      "digit =  7 \n",
      "\n",
      "00 00 00 0A 10 10 10 09 \n",
      "00 00 00 02 05 0A 10 0C \n",
      "00 00 00 00 00 06 10 02 \n",
      "00 00 00 00 00 0E 0A 00 \n",
      "00 01 08 0E 10 10 05 00 \n",
      "00 05 0A 08 10 0A 01 00 \n",
      "00 00 00 05 0E 01 00 00 \n",
      "00 00 00 0D 09 00 00 00 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_21096\\2622526565.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label = np.int(line[64] * 9.0)  # denormalize; like '5'\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_21096\\2622526565.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pxl = np.int(pxl * 16.0)  # denormalize\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "End autoencoder anomaly detection demo \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. show most anomalous item\n",
    "print(\"Largest reconstruction item / error: \")\n",
    "(idx,err) = err_list[0]\n",
    "print(\" [%4d]  %0.4f\" % (idx, err)) \n",
    "display_digit(data_ds, idx)\n",
    "\n",
    "print(\"\\nEnd autoencoder anomaly detection demo \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
