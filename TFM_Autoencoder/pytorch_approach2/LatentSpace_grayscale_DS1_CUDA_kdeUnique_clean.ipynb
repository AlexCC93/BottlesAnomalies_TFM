{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x000001B28AF4E340>\n",
      "GeForce RTX 2080 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../../../../BottleStoodUp_atNight/Positive/'        #For the work laptop\n",
    "data_dir = '../../../Images/BottleStoodUp_atNight/Positive'        #For the home laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_characteristics = transforms.Compose([transforms.Resize(255),\n",
    "#                                 transforms.CenterCrop(224),\n",
    "#                                 transforms.ToTensor()])\n",
    "\n",
    "transform_characteristics = transforms.Compose([transforms.Grayscale(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Resize(255),\n",
    "                                                transforms.CenterCrop(224)])\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform_characteristics)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the network in which the pre-trained model will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # 32, 1, 224, 224.  Batch size, input channels, shape of the image.\n",
    "        input_channels = 1              # number of channels of the input image\n",
    "        output_channels = 110           # ~= 224/2. Shape of the input image divided by 2 approximately.  \n",
    "        kernel_size = 9\n",
    "        padding_val = 1\n",
    "        stride_val = 5\n",
    "\n",
    "        \n",
    "        output_channels_layer2 = output_channels*2+5\n",
    "\n",
    "        output_channels_layer3 = output_channels_layer2*2\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride_val, padding=padding_val),         # input image channels, output channels, kernel size (filter). Dimension rseult: -> 15, 110, 44, 44. Batch size, channel output, output image shape.\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels, output_channels_layer2, kernel_size, stride=stride_val, padding=padding_val), # Dimension rseult: -> 15, 225, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels_layer2, output_channels_layer3, 8) # Dimension rseult: -> 15, 450, 1, 1\n",
    "        )\n",
    "        \n",
    "        # Initial dimension for this part of the model: 15 , 450, 1, 1\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(output_channels_layer3, output_channels_layer2, 8),  # Dimension rseult: -> 15, 225, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(output_channels_layer2, output_channels, kernel_size, stride=stride_val, padding=padding_val, output_padding=2), # Dimension rseult: -> 15, 110, 44, 44\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(output_channels, input_channels, kernel_size, stride=stride_val, padding=padding_val, output_padding=2), # Dimension rseult: -> 15, 1, 224, 224\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    " \n",
    "# Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "# Input [-1, +1] -> use nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(110, 225, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(225, 450, kernel_size=(8, 8), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(450, 225, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(225, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(110, 1, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(110, 225, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(225, 450, kernel_size=(8, 8), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(450, 225, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(225, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(110, 1, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1), output_padding=(2, 2))\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"../../../BottlesAnomalies_TFM/models/pytorchModels/PytorchModel_grayscale_withCUDA\"\n",
    "# For loading the model \n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the layers' weights of the model that has been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of layers that the loaded model has, is:  3\n"
     ]
    }
   ],
   "source": [
    "layers_weights_list = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        weights = m.weight\n",
    "        layers_weights_list.append(weights)\n",
    "print(\"The number of layers that the loaded model has, is: \", len(layers_weights_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the model layers' weights from one model to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_latentSpace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # 32, 1, 224, 224.  Batch size, input channels, shape of the image.\n",
    "        input_channels = 1              # number of channels of the input image\n",
    "        output_channels = 110           # ~= 224/2. Shape of the input image divided by 2 approximately. \n",
    "        kernel_size = 9\n",
    "        padding_val = 1\n",
    "        stride_val = 5\n",
    "\n",
    "        output_channels_layer2 = output_channels*2+5\n",
    "\n",
    "        output_channels_layer3 = output_channels_layer2*2\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride_val, padding=padding_val),         # input image channels, output channels, kernel size (filter). Dimension rseult: -> 15, 110, 44, 44. Batch size, channel output, output image shape.\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels, output_channels_layer2, kernel_size, stride=stride_val, padding=padding_val), # Dimension rseult: -> 15, 225, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels_layer2, output_channels_layer3, 8) # Dimension rseult: -> 15, 450, 1, 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"This is the forward function\")\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "    \n",
    "    def show_modules(self):\n",
    "        print(\"This is the show modules function\")\n",
    "        i = 0\n",
    "        for m in self.modules():\n",
    "            print(m)\n",
    "            print(\"i is: \", i)\n",
    "            print(\"print the next module\")\n",
    "            i = i +1\n",
    "            \n",
    "    def show_one_layer_weights(self, index):\n",
    "        print(\"This is the one layer show function\")\n",
    "        i = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if i == index:\n",
    "                    print(\"i is: \", i)\n",
    "                    print(\"The weights are: \", m.weight)\n",
    "                i = i +1\n",
    "    \n",
    "    def update_weights(self):\n",
    "        print(\"updating weights function\")\n",
    "        i = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight = layers_weights_list[i]\n",
    "                i = i +1\n",
    "\n",
    "                                        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder_latentSpace(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(110, 225, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(225, 450, kernel_size=(8, 8), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder = Autoencoder_latentSpace()\n",
    "model_encoder.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating weights function\n"
     ]
    }
   ],
   "source": [
    "model_encoder.update_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the one layer show function\n",
      "i is:  0\n",
      "The weights are:  Parameter containing:\n",
      "tensor([[[[-6.2896e-13, -7.6011e-13,  1.7919e-11,  ..., -1.3922e-11,\n",
      "            1.4555e-11, -1.7585e-11],\n",
      "          [-6.6017e-12, -3.8331e-11,  2.7313e-11,  ...,  6.4893e-13,\n",
      "           -2.9757e-12, -2.5010e-11],\n",
      "          [-2.6787e-13,  4.4217e-11, -1.3311e-11,  ..., -2.4048e-11,\n",
      "            2.7394e-11,  7.3216e-12],\n",
      "          ...,\n",
      "          [-1.0703e-12, -1.2735e-12, -1.4406e-12,  ..., -1.1631e-12,\n",
      "           -2.6765e-12,  8.1743e-12],\n",
      "          [ 2.9511e-15, -1.8266e-12, -1.2865e-12,  ..., -1.5972e-11,\n",
      "           -6.3332e-12,  1.6604e-11],\n",
      "          [-2.6665e-13, -1.7378e-11, -1.7329e-11,  ..., -1.1303e-12,\n",
      "           -2.9512e-12,  1.8606e-11]]],\n",
      "\n",
      "\n",
      "        [[[-5.3930e-02, -9.7788e-03, -3.9091e-02,  ..., -6.2419e-03,\n",
      "           -4.0850e-02, -3.4101e-02],\n",
      "          [-5.4251e-02,  8.6902e-02,  1.5842e-02,  ...,  9.3072e-02,\n",
      "            8.3332e-02,  5.0396e-02],\n",
      "          [ 7.5706e-02,  5.6760e-03,  1.0892e-01,  ...,  6.6947e-02,\n",
      "            8.4544e-02,  1.0581e-01],\n",
      "          ...,\n",
      "          [-9.0998e-02, -4.3780e-02, -3.4570e-02,  ..., -1.0113e-01,\n",
      "           -8.9242e-04,  3.4610e-03],\n",
      "          [ 3.3497e-02, -5.7344e-02, -8.6773e-02,  ...,  9.7919e-03,\n",
      "           -8.6456e-02, -3.0175e-02],\n",
      "          [-3.2598e-02, -1.0573e-01, -1.0496e-01,  ...,  4.1995e-02,\n",
      "           -3.4464e-02, -1.9126e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2538e-02, -1.1139e-02,  2.8268e-02,  ...,  4.6767e-02,\n",
      "           -1.3666e-02, -4.5353e-02],\n",
      "          [-2.7815e-02, -8.3336e-03, -2.8661e-02,  ...,  5.4976e-02,\n",
      "           -2.6878e-02,  7.4724e-05],\n",
      "          [-4.8552e-02,  2.4651e-02, -7.2031e-03,  ...,  3.4235e-02,\n",
      "            5.7204e-02, -3.9916e-02],\n",
      "          ...,\n",
      "          [-4.8549e-02, -2.2708e-04, -6.3290e-02,  ...,  7.5626e-03,\n",
      "            3.6204e-02, -2.6967e-02],\n",
      "          [-5.5188e-02, -6.2328e-02, -7.5424e-02,  ..., -3.4740e-02,\n",
      "            3.8996e-02, -1.2449e-02],\n",
      "          [-3.4861e-02,  3.8242e-02,  4.3533e-02,  ...,  4.9096e-02,\n",
      "            6.4148e-02,  3.4997e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0758e-28, -1.1177e-28, -1.0387e-28,  ..., -1.0360e-28,\n",
      "           -1.0221e-28, -1.0218e-28],\n",
      "          [-1.3473e-28, -1.3394e-28, -1.3183e-28,  ..., -1.3087e-28,\n",
      "           -1.2909e-28, -1.2847e-28],\n",
      "          [-1.3775e-28, -1.3449e-28, -1.3160e-28,  ..., -1.3141e-28,\n",
      "           -1.3398e-28, -1.3217e-28],\n",
      "          ...,\n",
      "          [-1.3724e-28, -1.3457e-28, -1.3790e-28,  ..., -1.3215e-28,\n",
      "           -1.3141e-28, -1.3447e-28],\n",
      "          [-1.4032e-28, -1.3552e-28, -1.3304e-28,  ..., -1.3545e-28,\n",
      "           -1.3306e-28, -1.3153e-28],\n",
      "          [-1.3594e-28, -1.2977e-28, -1.2989e-28,  ..., -1.2937e-28,\n",
      "           -1.3264e-28, -1.2906e-28]]],\n",
      "\n",
      "\n",
      "        [[[-1.6428e-02, -1.6234e-02,  8.4599e-03,  ..., -4.2894e-02,\n",
      "           -2.5113e-02,  4.9097e-03],\n",
      "          [ 5.6822e-02, -4.7165e-02, -1.8170e-03,  ..., -3.9467e-02,\n",
      "           -1.4190e-03,  1.9278e-02],\n",
      "          [-3.4694e-02, -5.3739e-03,  4.4451e-02,  ..., -8.4587e-04,\n",
      "            4.4206e-02, -4.2526e-04],\n",
      "          ...,\n",
      "          [-2.1763e-02, -6.7016e-04,  1.2295e-03,  ..., -5.3214e-02,\n",
      "            4.2589e-03,  3.0716e-02],\n",
      "          [-2.9931e-03,  5.4707e-04, -1.4029e-03,  ...,  3.9400e-03,\n",
      "           -3.0866e-02, -7.4491e-05],\n",
      "          [ 5.0216e-02, -4.4002e-03, -2.2109e-04,  ...,  3.2556e-02,\n",
      "            1.8147e-02, -4.8328e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1773e-09, -1.5568e-09, -1.8824e-09,  ...,  1.5962e-08,\n",
      "            1.6777e-08,  6.4108e-09],\n",
      "          [-2.7396e-08, -1.7277e-08, -1.9067e-08,  ...,  1.5055e-08,\n",
      "            1.2838e-08,  1.2711e-08],\n",
      "          [-1.5140e-08, -1.9049e-08, -1.6420e-08,  ...,  6.7300e-09,\n",
      "            9.8341e-09,  8.1646e-09],\n",
      "          ...,\n",
      "          [-2.9124e-09, -3.5406e-09, -3.5911e-09,  ..., -7.0038e-10,\n",
      "           -1.1222e-09, -3.9611e-10],\n",
      "          [-3.1147e-09, -3.2531e-09, -3.7352e-09,  ..., -8.1479e-10,\n",
      "           -2.1617e-10,  7.9433e-10],\n",
      "          [-2.0687e-09, -3.2533e-09, -2.5873e-09,  ...,  6.2158e-11,\n",
      "           -3.4023e-10,  4.0284e-10]]]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_encoder.show_one_layer_weights(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the created \"model_encoder\" model contains the weights of the encoder part of the pre-trained Autoencoder model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the KDE representation of the training set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Calculate KDE using sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#Get encoded output of input images = Latent space\n",
    "# encoded_images = model_encoder(images)\n",
    "encoded_images = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    X = dataset[i]\n",
    "    image_in_tensor = X[0]\n",
    "    image_in_tensor = image_in_tensor.cuda()            # Because GPU is being used\n",
    "    with torch.no_grad():\n",
    "        Y = model_encoder(image_in_tensor)  # should be same as X\n",
    "    encoded_images.append(Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert \"encoded_images\" to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "np_encoded_images = []\n",
    "for i in range (len(encoded_images)):\n",
    "    # np_conversion = encoded_images[i].detach().numpy()        # If not using GPU\n",
    "    np_conversion = encoded_images[i].cpu().detach().numpy()    # If using GPU\n",
    "    np_encoded_images.append(np_conversion)\n",
    "np_encoded_images = np.array(np_encoded_images)\n",
    "print(type(np_encoded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 450, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np_encoded_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, see above the shape of the representation of the original images has been lowered to (1, 1) as specified in the model structure. The number 450, on the other hand, corresponds to the channels of the image; this value started at 3 and layer by layer it incremented until reaching 450."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to flatten the data in order to apply kernel density on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "model_encoder_output_shape = (450,1,1)\n",
    "print(model_encoder_output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vector_shape = model_encoder_output_shape[0]*model_encoder_output_shape[1]*model_encoder_output_shape[2]\n",
    "encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in np_encoded_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(encoded_images_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function fits a kernel density estimation to the data that is provided, that is, to the \"encoded_images_vector\" variable. It does so using a Guassian kernel of bandwidth 0.2.\n",
    "\n",
    "The badnwidth parameter affects on how the selected kernel will fit each sample of the given data. For example for the case in which the kernel is a Gaussian distribution, the bandwidth parameter would affect in how thin or wide is the Gaussian distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the variable \"kde\" we have some numbers that are the result of fitting Gaussian functions to the given data points in the variable \"encoded_images_vecotr\". We will use the \"kde\" variable later for scoring with it, some given data points; the scoring will be given depending on how similar are the given data points to the ones that it had estimated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below, it is shown the kde values corresponding to each encoded sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53735411 305.53735411 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53735493 305.53735493 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 306.23048203 306.23048203 306.23048203 306.23048203 306.23048203\n",
      " 306.23048203 306.23048203 306.23075879 306.23075879 306.23048203\n",
      " 306.23048203 306.23048203 306.23048203 306.23048203 306.23048203\n",
      " 306.23048238 306.23048886 306.23048851 306.23048203 306.23048203\n",
      " 306.23048203 306.23048203 306.23048203 306.23048203 306.23048203\n",
      " 306.23048203 306.23048203 306.23075879 306.23075879 306.23048203\n",
      " 306.23048203 306.23048203 306.23048203 306.23048203 306.23048203\n",
      " 306.23048238 306.23048886 306.23048851 306.23048203 306.23048203\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.56614502\n",
      " 305.56614502 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.54654461 305.53733649 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53810758 305.53734086\n",
      " 305.53734246 305.54730989 305.53733485 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.62335008 305.57327655 305.61692301\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.5373381\n",
      " 305.62460816 305.62581278 305.53865984 305.53733485 305.53733485\n",
      " 305.53733485 305.53733485 305.58973003 305.54383362 305.58978376\n",
      " 305.54383408 305.53911089 305.53733485 305.53733485 305.53911089\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485 305.53733485\n",
      " 305.53739194 305.53733997 305.53733502 305.53734015 305.53733485\n",
      " 305.53733485 305.53733485 305.53733485 305.53733485]\n"
     ]
    }
   ],
   "source": [
    "density_vals = kde.score_samples(encoded_images_vector)\n",
    "print(density_vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above density values are pretty much the same among them. This has to do with the new model that was trained. The previous model from the \"Pt_latentSpace_DS1\" program, makes these value to be more different among them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the mean and standard deviation of these values are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg of the density values is:  305.6954621093459\n",
      "The stdev_density of the density values is:  0.28736754942052406\n"
     ]
    }
   ],
   "source": [
    "average_density = np.mean(density_vals)\n",
    "stdev_density = np.std(density_vals)\n",
    "print(\"The avg of the density values is: \", average_density)\n",
    "print(\"The stdev_density of the density values is: \", stdev_density)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it will be shown the density mean and std deviation of the set of anomalies samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anomalies = '../../../Images/BottleStoodUp_atNight/Anomalies2.0'      #This is for the home laptop\n",
    "# data_anomalies = '../../../Images/BottleStoodUp_atNight/Anomalies2.0'      #This is for the work laptop\n",
    "transform_characteristics = transforms.Compose([transforms.Grayscale(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Resize(255),\n",
    "                                                transforms.CenterCrop(224)])\n",
    "dataset_anomalies = datasets.ImageFolder(data_anomalies, transform=transform_characteristics)\n",
    "dataloader_anomalies = torch.utils.data.DataLoader(dataset_anomalies, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get encoded output of input images = Latent space\n",
    "encoded_anomalies_images = []\n",
    "\n",
    "for i in range(len(dataset_anomalies)):\n",
    "    X = dataset_anomalies[i]\n",
    "    image_in_tensor = X[0]\n",
    "    image_in_tensor = image_in_tensor.cuda()            # Because GPU is being used\n",
    "    with torch.no_grad():\n",
    "        Y = model_encoder(image_in_tensor)  # should be same as X\n",
    "    encoded_anomalies_images.append(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "np_encoded_anomaly_images = []\n",
    "for i in range (len(encoded_anomalies_images)):\n",
    "    # np_conversion = encoded_anomalies_images[i].detach().numpy()      # If GPU is not used\n",
    "    np_conversion = encoded_anomalies_images[i].cpu().detach().numpy()    # If GPU is used\n",
    "    np_encoded_anomaly_images.append(np_conversion)\n",
    "np_encoded_anomaly_images = np.array(np_encoded_anomaly_images)\n",
    "print(type(np_encoded_anomaly_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_anomaly_images_vector = [np.reshape(img_encoded, (out_vector_shape)) for img_encoded in np_encoded_anomaly_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304.99631974 295.75965058 284.1129431  302.93403716 270.180991\n",
      " 299.80925639]\n"
     ]
    }
   ],
   "source": [
    "density_vals_anomalies = kde.score_samples(encoded_anomaly_images_vector)\n",
    "print(density_vals_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg of the density values is:  292.96553299244846\n",
      "The stdev_density of the density values is:  12.22196281181243\n"
     ]
    }
   ],
   "source": [
    "average_density_anomalies = np.mean(density_vals_anomalies)\n",
    "stdev_density_anomalies = np.std(density_vals_anomalies)\n",
    "print(\"The avg of the density values is: \", average_density_anomalies)\n",
    "print(\"The stdev_density of the density values is: \", stdev_density_anomalies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the std deviation along with the mean of these density values will overlap the mean of the non-anomaly images. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the situation of the overlapping commented above, given a density value, it will be built a function that:\n",
    "- Assigns a percentage value according to the proximity to the mean of the non-anomaly images. For example: \n",
    "    - If the densitiy value is 305.69 (the mean of the density values of the non-anomaly images), then this density value should have 100% chance to be considered as non-anomaly.\n",
    "    - If the densitiy value is 305.4080 (the mean of the density values of the non-anomaly minus the std deviation of the same set), then this density value should have 50% chance to be considered as non-anomaly.\n",
    "- Assigns a percentage value according to the proximity to the mean of the anomaly images. For example: \n",
    "    - If the densitiy value is 292.96553299 (the mean of the density values of the anomaly images), then this density value should have 100% chance to be considered as an anomaly image.\n",
    "    - If the densitiy value is 305.18749299 (the mean of the density values of the anomaly plus the std deviation of the same set), then this density value should have 50% chance to be considered as an anomaly.\n",
    "- The two percentage values from above will be summed up assigning the following weights to the equation:\n",
    "\n",
    "        = perc_NOanomaly*0.75 + perc_anomaly*0.25\n",
    "        \n",
    "    More weight is assigned to the non-anomaly images because there are more samples of this kind of images.\n",
    "- After the weighted sum, the result will be subtracted from 100, to finally output the probability of an image to be an anomaly image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranges_mapper(value, leftMin, leftMax, rightMin, rightMax):\n",
    "    # if(value>leftMax):\n",
    "    #     return rightMax\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_kde2prob_list(input_list):\n",
    "    threshold_NOanomaly = average_density          # The mean of the density values corresponding to the non-anomaly images\n",
    "    std_dev_NOanomaly = stdev_density            # The std deviation of the density values corresponding to the non-anomaly images\n",
    "\n",
    "    threshold_anomaly = average_density_anomalies          # The mean of the density values corresponding to the anomaly images\n",
    "    std_dev_anomaly = stdev_density_anomalies            # The std deviation of the density values corresponding to the anomaly images\n",
    "    prob_score_list = []\n",
    "\n",
    "    for i in range (len(input_list)):\n",
    "        score_NOanomaly = input_list[i] - threshold_NOanomaly\n",
    "\n",
    "        perc_NOanomaly = ranges_mapper(abs(score_NOanomaly), 0, std_dev_NOanomaly, 100, 50)\n",
    "        if perc_NOanomaly<0:\n",
    "            perc_NOanomaly = 0\n",
    "        if perc_NOanomaly>100:\n",
    "            perc_NOanomaly = 100\n",
    "        \n",
    "\n",
    "        score_anomaly = input_list[i] - threshold_anomaly\n",
    "\n",
    "        perc_anomaly = ranges_mapper(abs(score_anomaly), 0, std_dev_anomaly, 100, 50)\n",
    "        if perc_anomaly<0:\n",
    "            perc_anomaly = 0\n",
    "        if perc_anomaly>100:\n",
    "            perc_anomaly = 100\n",
    "        \n",
    "        prob_score = 100 - (perc_NOanomaly*0.75+perc_anomaly*0.25)\n",
    "        prob_score_list.append(prob_score)\n",
    "    return prob_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_kde2prob(value):\n",
    "    threshold_NOanomaly = average_density          # The mean of the density values corresponding to the non-anomaly images\n",
    "    std_dev_NOanomaly = stdev_density            # The std deviation of the density values corresponding to the non-anomaly images\n",
    "\n",
    "    threshold_anomaly = average_density_anomalies          # The mean of the density values corresponding to the anomaly images\n",
    "    std_dev_anomaly = stdev_density_anomalies            # The std deviation of the density values corresponding to the anomaly images          # The std deviation of the density values corresponding to the anomaly images\n",
    "\n",
    "    score_NOanomaly = value - threshold_NOanomaly\n",
    "\n",
    "    perc_NOanomaly = ranges_mapper(abs(score_NOanomaly), 0, std_dev_NOanomaly, 100, 50)\n",
    "    if perc_NOanomaly<0:\n",
    "        perc_NOanomaly = 0\n",
    "    if perc_NOanomaly>100:\n",
    "        perc_NOanomaly = 100\n",
    "    # print(perc_NOanomaly)\n",
    "\n",
    "    score_anomaly = value - threshold_anomaly\n",
    "\n",
    "    perc_anomaly = ranges_mapper(abs(score_anomaly), 0, std_dev_anomaly, 100, 50)\n",
    "    if perc_anomaly<0:\n",
    "        perc_anomaly = 0\n",
    "    if perc_anomaly>100:\n",
    "        perc_anomaly = 100\n",
    "    # print(perc_anomaly)\n",
    "    prob_score = 100 - (perc_NOanomaly*0.75+perc_anomaly*0.25)\n",
    "  \n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePred(kde_value):\n",
    "    pred = 0\n",
    "    prob_anomaly = map_kde2prob(kde_value)/100\n",
    "    if prob_anomaly > 0.5:\n",
    "        pred = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prob of the kde value of being anomaly image is:  50.81342688849714\n",
      "Given the probability, it is actually predicted as: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"The prob of the kde value of being anomaly image is: \", map_kde2prob(average_density+stdev_density))\n",
    "print(\"Given the probability, it is actually predicted as:\", computePred(average_density+stdev_density))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the kde thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../../../Images/BottleStoodUp_atNight/Evaluation'      #This is for the home laptop\n",
    "transform_characteristics = transforms.Compose([transforms.Grayscale(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Resize(255),\n",
    "                                                transforms.CenterCrop(224)])\n",
    "\n",
    "dataset_test = datasets.ImageFolder(test_dir, transform=transform_characteristics)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "classes = ('non-anomaly','anomaly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Get encoded output of input images = Latent space\n",
    "encoded_test_imgs = []\n",
    "\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    X = dataset_test[i]\n",
    "    image_in_tensor = X[0]\n",
    "    image_in_tensor = image_in_tensor.cuda()     \n",
    "    with torch.no_grad():\n",
    "        Y = model_encoder(image_in_tensor)  # should be same as X\n",
    "    # np_conversion = Y.detach().numpy()\n",
    "    np_conversion = Y.cpu().detach().numpy()\n",
    "    encoded_test_imgs.append(np_conversion)\n",
    "np_encoded_test_images = np.array(encoded_test_imgs)\n",
    "print(type(np_encoded_test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "encoded_test_images_vector = [np.reshape(img, (out_vector_shape)) for img in np_encoded_test_images]\n",
    "print(len(encoded_test_images_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   173.96625642     39.7732893    -450.11832524   -453.57421735\n",
      "   -678.90549961   -954.2978617    -734.98576323   -544.07652391\n",
      "    -94.9319951    -364.23558479    248.96609166    207.47214304\n",
      "    271.96903131    135.62406001    -55.96028287    221.75658338\n",
      " -16829.9190714   -9634.41190478   -125.16659185     72.70706347\n",
      "    111.16769412    -86.62085592   -411.24273438   -336.50665239\n",
      "    112.86490421   -217.07878565   -226.40548349   -518.53001964\n",
      "   -599.95009973   -235.11120915]\n"
     ]
    }
   ],
   "source": [
    "density_vals_test = kde.score_samples(encoded_test_images_vector)\n",
    "print(density_vals_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shown values are REALLY strange. It was not expected to have negative values in the density numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test = map_kde2prob_list(density_vals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 96.47415067677507, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n"
     ]
    }
   ],
   "source": [
    "print(prob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexj\\.conda\\envs\\tfm_3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Grabbing only the first image of the anomalies dataset\n",
    "X = dataset_test[0]\n",
    "image_in_tensor = X[0]\n",
    " \n",
    "n_features = len(image_in_tensor[0])  # Get the size of one image of the anomaly images dataset. This is supposed to be 224\n",
    "for i in range(len(dataset_test)):\n",
    "    X = dataset_test[i]\n",
    "    image_in_tensor = X[0]\n",
    "    image_in_tensor = image_in_tensor.cuda() \n",
    "    ground_truth = X[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y = model_encoder(image_in_tensor)  # should be same as X\n",
    "    np_converted_encoded_img = Y.cpu().detach().numpy()\n",
    "    flattened = np.reshape(np_converted_encoded_img, (out_vector_shape))\n",
    "    density = kde.score_samples([flattened])[0]\n",
    "    prediction = computePred(density)\n",
    "    y_pred.append(prediction) # Save Prediction\n",
    "    y_true.append(ground_truth) # Save Truth\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it can be seen that all the images are predicted to be anomaly images. This is good for the TPR but it is terrible for the FPR metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
