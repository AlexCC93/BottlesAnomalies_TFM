{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexj\\AppData\\Local\\Temp\\ipykernel_11816\\706592764.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of our input images\n",
    "SIZE = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the training and testing datasets "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageDataGenerator.flow_from_directory: Takes the path to a directory and generates batches of augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "datagen = ImageDataGenerator(rescale=1./255) # Typically used for data augmentation. Here it is only rescaling the image pixels value to range 0 to 1 in decimals\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/BottleStoodUp_atNight/Positive/\",\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'          # Class used for working with Autoencoders\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/BottleStoodUp_atNight/Positive_val/\",\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/BottleStoodUp_atNight/Anomalies/\",\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the Autoencoder netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "#Decoder\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model. \n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 'withTFandGPU'\n",
    "model.save(f\"../../BottlesAnomalies_TFM/models/{model_version}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model that was previously saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/BottlesModel1')\n",
    "print(type(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all batches generated by the datagen and pick a batch for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to test the model. \n",
    "data_batch = []  #Capture all training batches as a numpy array\n",
    "img_num = 0\n",
    "while img_num <= train_generator.batch_index:   #gets each generated batch of size batch_size\n",
    "# while img_num <= len(train_generator):        #I think this should be the correct while clause\n",
    "    data = train_generator.next()\n",
    "    data_batch.append(data[0])\n",
    "    img_num = img_num + 1\n",
    "    \n",
    "print(\"number of batches are: \", img_num)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the first batch of images. Do also notice that the prediction is being carried out over the images we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_batch[0].shape)\n",
    "predicted = model.predict(data_batch[0])  \n",
    "print(predicted.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is 64 length because it corresponds to the 64 predicted images of a batch. Recall that the predictions are reconstructions of the input images, since we are working with the autoencoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check, view few images and corresponding reconstructions\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"input image\")\n",
    "plt.imshow(data_batch[0][image_number])\n",
    "plt.subplot(122)\n",
    "plt.title(\"reconstructed image\")\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the reconstruction is not that good, it is very blurry. However, we can see to generate negative artificial samples and see if these are capable of being detected as such."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the reconstruction error between our validation data (good/normal images) and the anomaly images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error = model.evaluate_generator(validation_generator)\n",
    "print(\"Recon. error for the validation (normal) data is: \", validation_error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here above we see a vector with two components which happen to be the same between them. This is because in the model, when we define the outputs, the loss function type and the metric are the same parameter, the mse (mean square error)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reconstruction error above should be very low, since the validation generator is full of normal images (good images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the reconstruction error here below, for the \"anomaly_generator\" should be higher, since this dataset is comprised of full anomaly images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_error = model.evaluate_generator(anomaly_generator)\n",
    "print(\"Recon. error for the anomaly data is: \", anomaly_error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that this error is slightly higher than the error in the validation dataset. This is good, but the difference is not that significant and this could result in wrong anomaly detections. Let's see some detections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs:\n",
    "- Initialize a vector for storing the reconstruction errors.\n",
    "- A for loop that traverses all the images contained in a batch.\n",
    "    - Take the one image of the batch.\n",
    "    - Add a new size dimension to the image.\n",
    "    - Make a prediction of the selected image with the model that was built.\n",
    "    - Evaluate the performance of the prediction, i.e. compute the prediction error.\n",
    "    - Append the reconstruction error into a list.\n",
    "- Compute the mean and std deviation of the error.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recon_error(batch_images):   \n",
    "    recon_error_list=[]\n",
    "    for im in range(0, batch_images.shape[0]-1):\n",
    "        img  = batch_images[im]\n",
    "        img = img[np.newaxis, :,:,:]\n",
    "        reconstruction = model.predict([[img]])\n",
    "        reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "        recon_error_list.append(reconstruction_error)   \n",
    "    average_recon_error = np.mean(np.array(recon_error_list))  \n",
    "    stdev_recon_error = np.std(np.array(recon_error_list)) \n",
    "    \n",
    "    return average_recon_error, stdev_recon_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average and std dev. of recon. error for positive (anomalies) and negative samples. \n",
    "For this let us generate a batch of images for each. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the first batch is selected for both, train and anomaly batches. For the anomalies case, notice that is not important to check this, since the anomaly dataset contains one batch only. For the train batch make sure to execute the below code block until the batch index is at 2, so that the next execution will select the batch index number 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (train_generator.batch_index != 2):\n",
    "    train_batch = train_generator.next()\n",
    "    print(train_generator.batch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_batch = anomaly_generator.next()\n",
    "print(anomaly_generator.batch_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = train_generator.next()[0]     # This is the uninfected images\n",
    "anomaly_batch = anomaly_generator.next()[0] # This is the parasitized images\n",
    "\n",
    "good_samples_values = calc_recon_error(train_batch)\n",
    "anomaly_values = calc_recon_error(anomaly_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean reconstruction error value for the good samples is: \", good_samples_values[0], \" with std deviation of: \", good_samples_values[1])\n",
    "print(\"The mean reconstruction error value for the anomalies samples is: \", anomaly_values[0], \" with std deviation of: \", anomaly_values[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information shown above, we can set a threshold for discriminating good samples from anomalies ones. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function does:\n",
    "- Sets the reconstruction error threshold.\n",
    "- Opens the given image path.\n",
    "- Resizes the image \n",
    "- Depending on the image size it adds a dimension to it. The image must be at the end of this of 3 dimensions in size.\n",
    "- Converts the image pixel values to float numbers, ranging from 0 to 1. \n",
    "- Adds a fourth dimension to the image.\n",
    "- Performs a prediction using the model and a given image.\n",
    "- Computes the prediction error.\n",
    "- Depending on the prediction error it prints the message to whether the image is an anomaly or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, input unknown images and sort as Good or Anomaly\n",
    "def check_anomaly(img_path):\n",
    "    reconstruction_error_threshold = 0.006 # Set this value based on the above exercise\n",
    "    img  = Image.open(img_path)\n",
    "    img = np.array(img.resize((128,128), Image.ANTIALIAS))\n",
    "    image_shape = img.shape\n",
    "    if(len(image_shape)==2):\n",
    "        img = np.dstack((img, img, img))\n",
    "    plt.imshow(img)\n",
    "    img = img / 255.\n",
    "    print(img.shape)\n",
    "    img = img[np.newaxis, :,:,:]\n",
    "    \n",
    "    reconstruction = model.predict([[img]])\n",
    "    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "\n",
    "    if reconstruction_error > reconstruction_error_threshold:\n",
    "        print(\"The image IS an anomaly\") \n",
    "    else:\n",
    "        print(\"The image is NOT an anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a couple of test images and verify whether they are reported as anomalies.\n",
    "import glob\n",
    "anomaly_file_paths = glob.glob(r'D:\\Alex2023\\TFM\\Images\\BottleStoodUp_atNight\\Anomalies\\Anomalies_samples\\*')\n",
    "positive_file_paths = glob.glob(r'D:\\Alex2023\\TFM\\Images\\BottleStoodUp_atNight\\Positive\\Positive_samples\\*')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=random.randint(0,len(anomaly_file_paths)-1)\n",
    "check_anomaly(anomaly_file_paths[num])\n",
    "print(\"This image directory is: \", anomaly_file_paths[num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the positive samples, these are NOT supposed to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=random.randint(0,len(positive_file_paths)-1)\n",
    "check_anomaly(positive_file_paths[num])\n",
    "print(\"This image directory is: \", positive_file_paths[num])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the specific tests above, the model has succesfully discriminated negative samples from the positive ones. However, varifying various images, it was seen that the performance is poor. Next we will label the images so that we can better analyze the results of all of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the anomalies samples with their correspondant labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_labels = np.ones(6)\n",
    "print(anomalies_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_samples = (anomaly_file_paths,anomalies_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the anomalies links: \", anomalies_samples[0])\n",
    "print(\"These are the anomalies labels: \", anomalies_samples[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the positive samples with their correspondant labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = np.zeros(len(positive_file_paths))\n",
    "print(positive_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = (positive_file_paths,positive_labels)\n",
    "print(\"These are the positive links: \", positive_samples[0])\n",
    "print(\"These are the positive labels: \", positive_samples[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new function baed on \"check_anomaly()\" that outputs directly the predicted label of the given image. This function is called predict_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(img_path):\n",
    "    reconstruction_error_threshold = 0.006\n",
    "    img  = Image.open(img_path)\n",
    "    img = np.array(img.resize((128,128), Image.ANTIALIAS))\n",
    "    image_shape = img.shape\n",
    "    if(len(image_shape)==2):\n",
    "        img = np.dstack((img, img, img))\n",
    "    img = img / 255.\n",
    "    img = img[np.newaxis, :,:,:]\n",
    "    reconstruction = model.predict([[img]])\n",
    "    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "    if reconstruction_error > reconstruction_error_threshold:\n",
    "        prediction = 1 \n",
    "    else:\n",
    "        prediction = 0\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=random.randint(0,len(positive_file_paths)-1)\n",
    "model_prediction = predict_label(positive_samples[0][num])\n",
    "model_label = int(positive_samples[1][num])\n",
    "print(\"The model predicts that the given image is: \", model_prediction)\n",
    "print(\"The original label of the given image is: \", model_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using some metrics for evaluating model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, SpecificityAtSensitivity\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the positive samples set. All images are NON-ANOMALIES bottles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see first, the precision of the training dataset, which consists on full positive images, i.e. image of bottles without anomalies, their labels are full zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive_samples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [] \n",
    "labels_list = [] \n",
    "for i in range(len(positive_file_paths)-1):\n",
    "    model_prediction = predict_label(positive_samples[0][i])\n",
    "    model_label = int(positive_samples[1][i])\n",
    "    predictions_list.append(model_prediction)  \n",
    "    labels_list.append(model_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Precision metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric simply divides true_positives by the sum of true_positives and false_positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "pre.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that:\n",
    "- Number of true positives elemnts: 2. The last two \"1\"s of the second vector.\n",
    "- Number of false positives: 1. The first \"1\" of the second vector. \n",
    "\n",
    "Then, the precision value of this prediction is: \n",
    "\n",
    "= 2/(2+1)\n",
    "\n",
    "= 0.66667"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have another example of the Precision function. This time it is using the \"sample_weight\" parameter. This is a mask, wherever there is a \"1\" the precision metric will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[1, 1, 1, 0])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final test of the Precision metric before applying it to the real data we are working with.\n",
    "\n",
    "Notice this case below. The ground truth, the first vector is telling us that there ALL of these samples are positive, i.e. these are normal bottles, bottles without any anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state([0, 0, 0, 0], [1, 0, 0, 1])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the precision value is zero. This is because:\n",
    "- True positives = 0 \n",
    "- False positives = 2\n",
    "\n",
    "Then Precision = 0/(0+2) = 0\n",
    "\n",
    "Interpreting this result: From all the samples that the model predicted to be anomaly bottles, all of them are wrong predictions. This is because we know that all the bottles were in reality, positive samples, not anomalies at all. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the effect of fliping the values of the vectors. Do we arrive to the same conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state([1 ,1 ,1, 1], [0, 1, 1, 0])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the precision value is 1. Since:\n",
    "- True positives = 2\n",
    "- False positives = 0\n",
    "\n",
    "Then Precision = 2/(2+0) = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be interpreted as: Since we have flipped the values, now 0 stands for bottles WITH anomalies and 1 stands for bottles WITHOUT anomalies. Hence, the model has 100% precision at the moment of predicting bottles WITHOUT anomalies. From all the bottles that the model predicted were bottles without anomalies (two bottles), all of them were correct predictions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tests above, it is clear that we want to see the precision of the model at the moment of predicting bottles WITH anomalies, hence we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state(labels_list, predictions_list)\n",
    "pre.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen the precision is 0%. That is because the model has predicted that some of the bottles DO HAVE anomalies where we know that in reality none of the bottles have any anomaly. \n",
    "\n",
    "It is also important to highlight that the optimal case would have been for example:\n",
    "\n",
    "Ground truth: [0,0,0,0]\n",
    "Predicted labels: [0,0,0,0]\n",
    "\n",
    "True positives = 0\n",
    "False positives = 0\n",
    "Precision = 0/(0+0) = 0\n",
    "\n",
    "Leading us to the conclusion that given this case, where all the samples of the ground truth are NOT anomaly bottles, the precision metric by its own might not be enough for explaining the truth performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state([0,0,0,0], [0, 0, 0, 0])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Recall metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = Recall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall metric tells us: out of all the positive examples, how many were predicted as positive? This metric is also called \"sensitiviy\" and TPR \"True Positive Rate\". It responds how good is the model at catching YESes?\n",
    "\n",
    "Remember we still working the dataset of full positive samples, i.e. bottles WITHOUT anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.update_state([0,1,1,0], [1, 0, 1, 0])\n",
    "re.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that:\n",
    "- Number of true positives elemnts: 1. The \"1\" of the third position of the second vector.\n",
    "- Number of false negatives: 1. The \"0\" of the second position of the second vector. \n",
    "\n",
    "Then, the recall value of this prediction is: \n",
    "\n",
    "= 1/(1+1)\n",
    "\n",
    "= 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this other example. Since we do not have any true positive, the recall metric will remain zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state([0,0,0,0], [0,0,1,0])\n",
    "re.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipping the values. Now the ANOMALIES bottles are represented by 0 and the NO-ANOMALIES bottles are represented by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state([1,1,1,1], [1,1,0,1])\n",
    "re.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that:\n",
    "- Number of true positives elemnts: 3.\n",
    "- Number of false negatives: 1. \n",
    "\n",
    "Recall = 3 / (3+1) = 0.75\n",
    "\n",
    "We have a recall of 75% of detecting bottles WITHOUT anomalies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check with the dataset of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state(labels_list, predictions_list)\n",
    "re.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have any true positive the recall value is still 0 for this case. \n",
    "\n",
    "And it happens the same as in the Precision metric. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state([0,0,0,0], [0,0,0,0])\n",
    "re.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is shown a fully correct prediction. But still the recall value is zero. This is because no positive sample is present in the ground truth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Specificity metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specificity metric is the opposite as sensitivity, it tells us how good is the model at catching NOes. It measures the proportion of actual negatives that are correctly identified as such.\n",
    "\n",
    "Specificity = (tn / (tn + fp))\n",
    "\n",
    "As we are working in here with full NOes, that is bottles WITHOUT anomalies, perhaps this would be a good metric for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = SpecificityAtSensitivity(0.5)  # Not completely sure why to input 0.5 for specificity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.update_state([0,1,1,0], [1, 0, 1, 0])\n",
    "spec.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above consists on:\n",
    "- True negative detections: 1.\n",
    "- False positive detections: 1.\n",
    "\n",
    "Specificity = 1/(1+1) = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it does not work very well with other examples. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.reset_state()\n",
    "spec.update_state([0,0,0,0], [0,1,0,0])\n",
    "spec.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result should be:\n",
    "- True negative detections: 3.\n",
    "- False positive detections: 1.\n",
    "\n",
    "Specificity = 3/(3+1) = 0.75"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, for the moment the computation of the specificity will be made with an alternative function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the sklearn library for specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix([0,1,1,0], [1, 0, 1, 0]).ravel()\n",
    "specificity_val = tn/(tn+fp)\n",
    "if(math.isnan(specificity_val)):\n",
    "    specificity_val = 0\n",
    "print(\"The specificity value is: \", specificity_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above works according to the expected. See another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix([0,0,0,0], [0,1,0,0]).ravel()\n",
    "specificity_val = tn/(tn+fp)\n",
    "if(math.isnan(specificity_val)):\n",
    "    specificity_val = 0\n",
    "print(\"The specificity value is: \", specificity_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above as well, it is the expected specificity value. Now try with the actual data of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(labels_list, predictions_list).ravel()\n",
    "specificity_val = tn/(tn+fp)\n",
    "if(math.isnan(specificity_val)):\n",
    "    specificity_val = 0\n",
    "print(\"The specificity value is: \", specificity_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has 53.93% of specificity metric detection. This means that, when it comes to detect normal bottles, that is, bottles without anomalies, the model performs it well in 53.93% of the predicitons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the frequency with which y_pred matches y_true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.update_state([0,1,1,0], [1, 0, 1, 0])\n",
    "acc.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see:\n",
    "- The last two numbers match values.\n",
    "- There are four numbers in total.\n",
    "\n",
    "Accuracy = 2/4 = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to the dataset we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.reset_state()\n",
    "acc.update_state(labels_list, predictions_list)\n",
    "acc.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs a 53.93% of accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the predicted and true labels we can generate the confustion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = confusion_matrix(labels_list, predictions_list,normalize='pred') #Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(labels_list, predictions_list)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to plot the confusion matrix in a very comprehensive way, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                result.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     result.flatten()/np.sum(result)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]        #To print the group names as well \n",
    "# labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "#           zip(group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "categories = ['Positive bottles', 'Anomaly bottles']\n",
    "sn.heatmap(result, annot=labels, fmt='', xticklabels=categories,yticklabels=categories, cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the model correctly classifies the positive samples, that is, the bottles without anomalies (96 correctly classified). On the other hand, the classifications for anomaly bottles, the model incorrectly classifies 82 samples as anomalies when it should be positive samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The F1 score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is used for creating a great compromise between recall and precision so that you don’t get extreme cases. Consider this example: \n",
    "\n",
    "5 pictures of dog, 995 pictures of anything else (imbalanced). We get a classifier running, and it correctly gets one of those dog pictures classified, but calls everything else not-a-dog. That means there are 4 incorrectly classified pictures of dogs. Recall is 0.2 (pretty bad) and precision is 1.0 (perfect), but accuracy, clocking in at 0.999, isn’t reflecting how badly the model did at catching those dog pictures; F1 score, equal to 0.33, is capturing the poor balance between recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = f1_score([1,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1], average='weighted')\n",
    "f1 = f1_score([1,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1])\n",
    "print(f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above, had a precision of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state([1,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1])\n",
    "pre.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a recall of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state([1,0,0,0,0,0,0,0,0,0], [1,1,1,1,1,1,1,1,1,1])\n",
    "re.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the F1 score presents a balance between the extreme cases of precision and recall.\n",
    "\n",
    "Now, let's see the F1 score for the data of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(labels_list, predictions_list)\n",
    "print(f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since precision and recall metrics for this case were equal to zero, then the F1 score for this dataset is as well zero. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "- True Positive Rate.\n",
    "- False Positive Rate.\n",
    "\n",
    "As this metric works with a proabability output value, we will have to write a new function for our predictions to output one such value. For the moment we have the \"predict_label()\" function that, working with a threshold reconstruction error, outputs a prediction of the label, either \"1\" or \"0\". Hence, given this situation we have the following possibilites to approach this task:\n",
    "\n",
    "- Option 1: Map the \"reconstruction_error\" of each prediction to a probability percentage that ranges from 0 to 100%, where, of course, 100% will correspond to a completely certain prediction. \n",
    "- Option 2: Considering the \"reconstruction_error_threshold\" as the threshold to be varying for the ROC to be built. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Option 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the error values that the predictions output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_error(img_path):\n",
    "    img  = Image.open(img_path)\n",
    "    img = np.array(img.resize((128,128), Image.ANTIALIAS))\n",
    "    image_shape = img.shape\n",
    "    if(len(image_shape)==2):\n",
    "        img = np.dstack((img, img, img))\n",
    "    img = img / 255.\n",
    "    img = img[np.newaxis, :,:,:]\n",
    "    reconstruction = model.predict([[img]])\n",
    "    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "    return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = []\n",
    "for i in range(len(positive_file_paths)-1):\n",
    "    error_value = output_error(positive_samples[0][i])\n",
    "    errors_list.append(error_value)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0. # this is the value where you want the data to appear on the y-axis.\n",
    "plt.plot(errors_list, np.zeros_like(errors_list) + val, 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some description of the errors list is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_recon_error = np.mean(np.array(errors_list))  \n",
    "stdev_recon_error = np.std(np.array(errors_list)) \n",
    "min_prob = np.min(errors_list)\n",
    "max_prob = np.max(errors_list)\n",
    "\n",
    "print(\"The average of the errors list is: \", average_recon_error)\n",
    "print(\"The standard deviation of the errors list is: \", stdev_recon_error)\n",
    "print(\"The min value of the errors list is: \", min_prob)\n",
    "print(\"The max value of the errors list is: \", max_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the data points of the errors, the average and the standard deviation of it, we can map the errors to be a percentage probability in the following manner:\n",
    "\n",
    "- 0.004726, which is the minimum error, can represent a 100% probability of predicting the image as a non-anomaly bottle.\n",
    "- 0.007921, which is the maximum error, can represent a 60% probability of predicting the image as a non-anomaly bottle. \n",
    "\n",
    "The above conclusion is stated because we know we are working with only non-anomaly bottles. Hence, those error values correspond, all of them, to non-anomaly bottles; even the image that outputed the maximum error value must be predicted to be a non-anomaly bottle. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, it would be interesting to see as well a plot of the error values of the images that are indeed anomaly bottles. Let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list_anomalies = []\n",
    "for i in range(len(anomaly_file_paths)-1):\n",
    "    error_value = output_error(anomalies_samples[0][i])\n",
    "    errors_list_anomalies.append(error_value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0. # this is the value where you want the data to appear on the y-axis.\n",
    "plt.plot(errors_list_anomalies, np.zeros_like(errors_list_anomalies) + val, 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_recon_error = np.mean(np.array(errors_list_anomalies))  \n",
    "stdev_recon_error = np.std(np.array(errors_list_anomalies)) \n",
    "min_prob = np.min(errors_list_anomalies)\n",
    "max_prob = np.max(errors_list_anomalies)\n",
    "\n",
    "print(\"The average of the errors list is: \", average_recon_error)\n",
    "print(\"The standard deviation of the errors list is: \", stdev_recon_error)\n",
    "print(\"The min value of the errors list is: \", min_prob)\n",
    "print(\"The max value of the errors list is: \", max_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot both of the errors values in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "val = 0. # this is the value where you want the data to appear on the y-axis.\n",
    "plt.plot(errors_list, np.zeros_like(errors_list) + val, 'x')\n",
    "plt.plot(errors_list_anomalies, np.zeros_like(errors_list_anomalies) + val, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we have a problem. The error values of the anomaly bottles are mixed with the error values of the non-anomaly bottles. It is difficult to draw a line to divide these two categories. However, we can consider of focusing only in the part where most of the non-anomaly bottles error values lie on, that is, if an error value lies into the Guassian distribution of the non-anomaly errors distribution, then it will have more probability of being predicted as such. If the error value lies outside the Gaussian distribution, then the percentage of being detected as such should decrease."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code some function that if:\n",
    "- The error value is 0.006, which is exactly the average of the non-anomaly distribution, then it should have a 100% probability of being detected as such.\n",
    "- The error value is 0.006 +/- 0.00049 then the probability of being detected as a non-anomaly bottle is 0%.\n",
    "\n",
    "The above data is based on the Gaussian distribution of the non-anomaly bottles errors.\n",
    "\n",
    "REMINDER: THE ABOVE STATEMENT WAS CHANGED TO:\n",
    "\n",
    "    - The error value is 0.006, which is exactly the average of the non-anomaly distribution, then it should have a 0% probability of being detected as such.\n",
    "    - The error value is 0.006 +/- 0.00049 then the probability of being detected as a non-anomaly bottle is 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranges_mapper(value, leftMin, leftMax, rightMin, rightMax):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_error2prob(value):\n",
    "    # Calculate the mean and standard deviation\n",
    "    mean = 0.006\n",
    "    std_dev = 0.00049\n",
    "    aux_score = abs(value - mean)\n",
    "    if aux_score > 0.00049:\n",
    "        aux_score = 0.00049\n",
    "    # prob_score = ranges_mapper(aux_score, 0, std_dev, 100,0)              # This is considering that the 100% refers to a non-anomaly sample\n",
    "    prob_score = ranges_mapper(aux_score, 0, std_dev, 0,100)                # This is considering that the 100% refers to an anomaly sample\n",
    "    return prob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_error2prob(0.006))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the function that maps the reconstruction model error to a probability value, we create a list with all the probabilities values of the predictions. Remember that we are working with only the possitive samples in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_error2prob_list(input_list):\n",
    "    # Calculate the mean and standard deviation\n",
    "    mean = 0.006\n",
    "    std_dev = 0.00049\n",
    "    prob_score_list = []\n",
    "    for i in range (len(input_list)):\n",
    "        aux_score = abs(input_list[i] - mean)\n",
    "        if aux_score > 0.00049:\n",
    "            aux_score = 0.00049\n",
    "        # prob_score = ranges_mapper(aux_score, 0, std_dev, 100,0)              # This is considering that the 100% refers to a non-anomaly sample\n",
    "        prob_score = ranges_mapper(aux_score, 0, std_dev, 0,100)                # This is considering that the 100% refers to an anomaly sample\n",
    "        prob_score_list.append(prob_score)\n",
    "    return prob_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(errors_list)\n",
    "preds_probs = np.array(map_error2prob_list(errors_list))\n",
    "preds_probs = preds_probs/100\n",
    "print(\"The prediction probabilities list for the positive samples is: \")\n",
    "print(preds_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "auc_score = roc_auc_score(positive_samples[1], preds_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary for the ROC score to be computed that there exist in the test set, at least one sample of each class. As in this case we are trying to work with a single-class test set, then the above error is shown. Trying to create a new test set where we mix positive and negative samples.\n",
    "First, let's take 40 samples out of the positive samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples_list = []\n",
    "for i in range (len(positive_file_paths)):\n",
    "    img_link = positive_samples[0][i]\n",
    "    img_label = positive_samples[1][i]\n",
    "    positive_samples_list.append((img_link, img_label)) \n",
    "print(\"The total number of samples in the positive samples list is: \", len(positive_samples_list))\n",
    "\n",
    "portion_positive_samples = []\n",
    "for i in range (40):\n",
    "    index = random.randint(0, len(positive_file_paths)-1)\n",
    "    portion_positive_samples.append(positive_samples_list[index])\n",
    "print(\"The portion that was took of the positive samples list is: \", len(portion_positive_samples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now mix these 40 sample with the anomalies samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_samples_list = []\n",
    "for i in range (len(anomaly_file_paths)):\n",
    "    img_link = anomalies_samples[0][i]\n",
    "    img_label = anomalies_samples[1][i]\n",
    "    anomalies_samples_list.append((img_link, img_label)) \n",
    "len(anomalies_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test = []\n",
    "samples_test = portion_positive_samples\n",
    "for i in range(len(anomalies_samples_list)):\n",
    "    samples_test.append(anomalies_samples_list[i])\n",
    "print(len(samples_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a test set that is comprised of negative and positive samples. Over this test set we will predict the probability that each of these samples have of being or not an anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list = []\n",
    "for i in range(len(samples_test)):\n",
    "    error_value = output_error(samples_test[i][0])\n",
    "    errors_list.append(error_value)\n",
    "print(len(errors_list))\n",
    "preds_probs = np.array(map_error2prob_list(errors_list))\n",
    "preds_probs = preds_probs/100\n",
    "print(\"The prediction probabilities list for the positive samples is: \")\n",
    "print(preds_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the list of the probabilities of the samples to be a non-anomaly image. The higher the value the more likely for them to be non-anomally bottles. \n",
    "\n",
    "REMINDER: THE ABOVE STATEMENT WAS CHANGED TO:\n",
    "\n",
    "    - The above is the list of the probabilities of the samples to be an anomaly image. The higher the value the more likely for them to be an anomally bottle. \n",
    "Extracting now only the labels values of the test set that was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_labels = []\n",
    "for i in range(len(samples_test)):\n",
    "    label = samples_test[i][1]\n",
    "    test_set_labels.append(label)\n",
    "print(test_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score = roc_auc_score(test_set_labels, preds_probs)\n",
    "print('AUROC = %.3f' % (auc_score))\n",
    "fpr, tpr, thresholds = roc_curve(test_set_labels, preds_probs)\n",
    "print(\"The fpr is: \", fpr)\n",
    "print(\"The tpr is: \", tpr)\n",
    "print(\"The thresholds are: \", thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, marker='.', label='AUROC = %0.3f' % auc_score)\n",
    "# Title\n",
    "plt.title('ROC Plot')\n",
    "# Axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# Show legend\n",
    "plt.legend() # \n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the FPR and TPR computation vs what was computed with the scikitlearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_label_onProb(probs_list):\n",
    "    pred_based_onProb = []\n",
    "    # prob_threshold = 0.9979607\n",
    "    prob_threshold = 0.03469472\n",
    "    for i in range (len(probs_list)):\n",
    "        if probs_list[i] > prob_threshold:\n",
    "            prediction = 1                          ## Stands for classifiaction of anomaly bottle\n",
    "        else:\n",
    "            prediction = 0\n",
    "        pred_based_onProb.append(prediction)\n",
    "    return pred_based_onProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_basedOnProbs = pred_label_onProb(preds_probs)\n",
    "print(predictions_basedOnProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_set_labels, predictions_basedOnProbs)\n",
    "print(conf_matrix)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf_matrix.flatten()/np.sum(conf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]        #To print the group names as well \n",
    "# labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "#           zip(group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "categories = ['Positive bottles', 'Anomaly bottles']\n",
    "sn.heatmap(conf_matrix, annot=labels, fmt='', xticklabels=categories,yticklabels=categories, cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TPR metric, manually computed, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state(test_set_labels, predictions_basedOnProbs)\n",
    "print(\"The TPR value is: \",re.result().numpy()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FPR metric, manually computed, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tp = conf_matrix[1, 1]\n",
    "_fn = conf_matrix[1, 0]\n",
    "_fp = conf_matrix[0, 1]\n",
    "_tn = conf_matrix[0, 0]\n",
    "_tpr = _tp/(_tp+_fn)\n",
    "_fpr = _fp / (_tn + _fp)\n",
    "print(\"The TPR values are: \", _tpr)\n",
    "print(\"The FPR value is: \", _fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The fpr is: \", fpr)\n",
    "print(\"The tpr is: \", tpr)\n",
    "print(\"The thresholds are: \", thresholds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that working now the values computed with the scikitlearn library matches the manually obtained values of FPR and TPR. There is a little difference in some of the thresholds values but this is beleived to be due to the decimals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the anomalies samples set. All images are ANOMALIES bottles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [] \n",
    "labels_list = [] \n",
    "for i in range(len(anomaly_file_paths)-1):\n",
    "    model_prediction = predict_label(anomalies_samples[0][i])\n",
    "    model_label = int(anomalies_samples[1][i])\n",
    "    predictions_list.append(model_prediction)  \n",
    "    labels_list.append(model_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Precision metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.reset_state()\n",
    "pre.update_state(labels_list, predictions_list)\n",
    "pre.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the ground truth list: \", labels_list)\n",
    "print(\"This is the prediction list: \", predictions_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that from all the elements that the model has predicted to be positive, all of them are correct, that is why the precision value of this model is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Recall metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = Recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.reset_state()\n",
    "re.update_state(labels_list, predictions_list)\n",
    "re.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the ground truth list: \", labels_list)\n",
    "print(\"This is the prediction list: \", predictions_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that because of the two false negatives, the recall value becomes:\n",
    "\n",
    "recall = 3 / (3+2) = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Specificity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(labels_list, predictions_list).ravel()\n",
    "specificity_val = tn/(tn+fp)\n",
    "if(math.isnan(specificity_val)):\n",
    "    specificity_val = 0\n",
    "print(\"The specificity value is: \", specificity_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has 0% of specificity metric detection. This makes sense since in the dataset there does not exist any bottle without anomaly, i.e. no true negative value could be included for the computation. In this kind of dataset, a better metric to look at is the recall metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.reset_state()\n",
    "acc.update_state(labels_list, predictions_list)\n",
    "acc.result().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs a 60% of accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the predicted and true labels we can generate the confustion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result = confusion_matrix(labels_list, predictions_list,normalize='pred') #Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(labels_list, predictions_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to plot the confusion matrix in a very comprehensive way, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                result.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     result.flatten()/np.sum(result)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]          #To print the group names as well \n",
    "# labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "#           zip(group_counts,group_percentages)]                    #For printing without the group names labels                    \n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "categories = ['Positive bottles', 'Anomaly bottles']\n",
    "sn.heatmap(result, annot=labels, fmt='', xticklabels=categories,yticklabels=categories, cmap='Blues')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the model correctly classifies the anomaly bottles (3 true positives). On the other hand, the classifications for bottles without anomalies, the model incorrectly classifies 2 of them; it classifies as bottles without anomalies where in reality these are bottles with anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
