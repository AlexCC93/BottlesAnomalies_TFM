{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../Images/BottleStoodUp_atNight/Positive'      #This is for the home laptop\n",
    "# data_dir = '../../../../BottleStoodUp_atNight/Positive/'        #For the work laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_characteristics = transforms.Compose([transforms.Resize(255),\n",
    "#                                 transforms.CenterCrop(224),\n",
    "#                                 transforms.ToTensor()])\n",
    "transform_characteristics = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Resize(255),\n",
    "                                                transforms.CenterCrop(224)])\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform_characteristics)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only the encoder part of the nertwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_latentSpace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # N, 1, 28, 28\n",
    "        # 32, 3, 224, 224\n",
    "        input_channels = 3              # number of channels of the input image\n",
    "        output_channels = 110           # ~= 224/2. Shape of the input image \n",
    "        kernel_size = 9\n",
    "        padding_val = 1\n",
    "        stride_val = 5\n",
    "\n",
    "        \n",
    "        output_channels_layer2 = output_channels*2+5\n",
    "\n",
    "        output_channels_layer3 = output_channels_layer2*2\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride_val, padding=padding_val),         # input image channels, output channels, kernel size (filter). Dimension rseult: -> N, 110, 44, 44\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels, output_channels_layer2, kernel_size, stride=stride_val, padding=padding_val), # -> N, 225, 8, 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_channels_layer2, output_channels_layer3, 8) # -> N, 450, 1, 1\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder_latentSpace(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 110, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(110, 225, kernel_size=(9, 9), stride=(5, 5), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(225, 450, kernel_size=(8, 8), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder_latentSpace()\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to use this new model. This will generate a feature vector or also called the latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate KDE using sklearn\n",
    "from sklearn.neighbors import KernelDensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to training loop video\n",
    "num_epochs = 10\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in dataloader:     # This iterates over the batches of images.\n",
    "        \n",
    "        # Forward pass\n",
    "        recon = model(img)          # The entire batch of images is passed to the model\n",
    "        loss = criterion(recon, img)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()       # Sets the gradients of all optimized torch.Tensors to zero\n",
    "        loss.backward()             # The accumulation (or sum) of all the gradients is calculated     \n",
    "        optimizer.step()            # can be called once the gradients are computed \n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
