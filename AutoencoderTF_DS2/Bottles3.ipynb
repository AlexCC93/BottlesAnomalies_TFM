{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "# from keras import layers\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) == 0:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "else:\n",
    "    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)       # Will print the log information of every execution of tensorflow, mainly about "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the training and testing datasets "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageDataGenerator.flow_from_directory: Takes the path to a directory and generates batches of augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_crop(img, start_y, start_x, desired_height, desired_width):\n",
    "  if K.image_data_format() == 'channels_last':\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy = desired_height #input desired output size\n",
    "    dx = desired_width #input desired output size\n",
    "    # start_y = (height-dy)//2\n",
    "    # start_x = (width-dx)//2\n",
    "    return img[start_y:start_y+dy, start_x:(dx+start_x), :]\n",
    "  else:\n",
    "      assert img.shape[0] == 3\n",
    "      height, width = img.shape[1], img.shape[2]\n",
    "      dy = desired_height #input desired output size\n",
    "      dx = desired_width #input desired output size\n",
    "      # start_y = (height-dy)//2\n",
    "      # start_x = (width-dx)//2\n",
    "      return img[:,start_y:start_y + dy, start_x:(dx + start_x)]\n",
    "\n",
    "def crop_generator(batches, start_y, start_x, crop_height, crop_width):\n",
    "    '''\n",
    "    Take as input a Keras ImageGen (Iterator) and generate\n",
    "    crops from the image batches generated by the original iterator\n",
    "    '''\n",
    "    # print(\"I'm in the crop_generataor function\")\n",
    "    while True:\n",
    "      batch_x, batch_y = next(batches)  # In batch_x there are the images, in batch_y there are the labels\n",
    "    #   print(\"hello\")\n",
    "    #   print('the shape of tensor batch_x is:', batch_x.shape)\n",
    "    #   print('batch_y is:', batch_y)\n",
    "      \n",
    "      if K.image_data_format() == 'channels_last':\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_height, crop_width, 3))\n",
    "      else:\n",
    "        batch_crops = np.zeros((batch_x.shape[0], 3, crop_height, crop_width))\n",
    "      for i in range(batch_x.shape[0]):\n",
    "        #   print(\"THe shape of a single image before the cropping is: \", batch_x[i].shape)\n",
    "          batch_crops[i] = my_crop(batch_x[i], start_y, start_x, crop_height, crop_width)\n",
    "      yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 295 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 64\n",
    "batch_size = 32\n",
    "datagen = ImageDataGenerator(rescale=1./255) # Typically used for data augmentation. Here it is only rescaling the image pixels value to range 0 to 1 in decimals\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/june5Night/Positive/\",         # For home laptop\n",
    "    # \"../../../BottleStoodUp_atNight/Positive/\",          # For work laptop\n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='input'\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/june5Night/Positive_val/\",     # For home laptop\n",
    "    # \"../../../BottleStoodUp_atNight/Positive_val/\",           # For work lpatop   \n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='input'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/june5Night/Anomalies2.0/\",         # For home laptop\n",
    "    # \"../../../BottleStoodUp_atNight/Anomalies2.0/\",           # For work lpatpo\n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='binary'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set_generator = datagen.flow_from_directory(\n",
    "    \"../../Images/june5Night/Evaluation/\",       # For home laptop\n",
    "    # \"../../../BottleStoodUp_atNight/Evaluation/\",             # For work laptop  \n",
    "    batch_size=batch_size,\n",
    "    shuffle = False,\n",
    "    class_mode='binary'\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 160\n",
    "WIDTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crops = crop_generator(train_generator, 65, 69, HEIGHT,WIDTH)      # top, left, height, width\n",
    "val_crops = crop_generator(validation_generator, 65, 69, HEIGHT,WIDTH)      # top, left, height, width\n",
    "anomaly_crops = crop_generator(anomaly_generator, 65, 69, HEIGHT,WIDTH)      # top, left, height, width\n",
    "test_crops = crop_generator(test_set_generator, 65, 69, HEIGHT,WIDTH)      # top, left, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_crops))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIlling the batches into lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing only one image of one batch of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(32, 160, 128, 3)\n",
      "(7, 160, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "list_batches_training = []\n",
    "for i in range(len(train_generator)):\n",
    "    images_in_batch = next(train_crops)[0]\n",
    "    print(images_in_batch.shape)\n",
    "    list_batches_training.append(images_in_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(list_batches_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 160, 128, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "list_batches_val_pos = []\n",
    "for i in range(len(validation_generator)):\n",
    "    images_in_batch = next(val_crops)[0]\n",
    "    print(images_in_batch.shape)\n",
    "    list_batches_val_pos.append(images_in_batch)\n",
    "print(len(list_batches_val_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 160, 128, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "list_batches_anomaly = []\n",
    "for i in range(len(anomaly_generator)):\n",
    "    images_in_batch = next(anomaly_crops)[0]\n",
    "    print(images_in_batch.shape)\n",
    "    list_batches_anomaly.append(images_in_batch)\n",
    "print(len(list_batches_anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 160, 128, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "list_batches_test = []\n",
    "for i in range(len(test_set_generator)):\n",
    "    images_in_batch = next(test_crops)[0]\n",
    "    print(images_in_batch.shape)\n",
    "    list_batches_test.append(images_in_batch)\n",
    "print(len(list_batches_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the Autoencoder netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 22:24:45.487360: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-06-10 22:24:45.511764: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz\n",
      "2023-06-10 22:24:45.512196: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f37f8000db0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-10 22:24:45.512210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-06-10 22:24:45.623666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.624070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3e72520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-10 22:24:45.624084: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 with Max-Q Design, Compute Capability 7.5\n",
      "2023-06-10 22:24:45.624220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.624528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.23GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2023-06-10 22:24:45.624558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-10 22:24:45.624570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-10 22:24:45.624580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-10 22:24:45.624591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-10 22:24:45.624601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-10 22:24:45.624611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-10 22:24:45.624621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-10 22:24:45.624669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.624997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.625283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-06-10 22:24:45.625310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-10 22:24:45.625712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-10 22:24:45.625721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2023-06-10 22:24:45.625727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2023-06-10 22:24:45.625806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.626133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-10 22:24:45.626439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6451 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10, 8, 1)\n",
      "(None, 40)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 160, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 16, 4)         580       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 8, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 8, 1)          37        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                3240      \n",
      "=================================================================\n",
      "Total params: 35,217\n",
      "Trainable params: 35,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 40  # Number of latent dimension parameters\n",
    "\n",
    "input_img = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D( (2, 2), padding='same')(x)\n",
    "x = Conv2D(1, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "print(shape_before_flattening)\n",
    "x = Flatten()(x)\n",
    "x = Dense(80, activation='relu')(x)        # Activation function is supposed to be none, i.e the output is the same as the input\n",
    "\n",
    "Z = Dense(latent_dim)(x)\n",
    "print(K.int_shape(Z))\n",
    "\n",
    "encoder = Model(input_img,Z)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 40)]              0         \n",
      "_________________________________________________________________\n",
      "intermediate_decoder (Dense) (None, 320)               13120     \n",
      "_________________________________________________________________\n",
      "original_decoder (Dense)     (None, 320)               102720    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 8, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 10, 8, 3)          111       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 20, 16, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 20, 16, 3)         84        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 40, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 40, 32, 3)         84        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 80, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 80, 64, 3)         84        \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 160, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 116,203\n",
      "Trainable params: 116,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder takes the latent distribution sample as input\n",
    "decoder_input = Input(K.int_shape(Z)[1:])\n",
    "x = Dense(10*8*4, activation='relu', name=\"intermediate_decoder\", input_shape=(latent_dim,))(decoder_input)\n",
    "# Expand to 784 total pixels\n",
    "x = Dense(320, activation='sigmoid', name=\"original_decoder\")(x)\n",
    "x = Reshape((10,8,4),input_shape=(320,))(x)\n",
    "\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "x = Conv2DTranspose(3, (3, 3), padding='same')(x)\n",
    "x = UpSampling2D( (2, 2))(x)\n",
    "\n",
    "# decoder model statement\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# apply the decoder to the sample from the latent distribution\n",
    "z_decoded = decoder(Z)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 160, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 80, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 80, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 40, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 40, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 16, 4)         580       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 8, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 8, 1)          37        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 160, 128, 3)       116203    \n",
      "=================================================================\n",
      "Total params: 151,420\n",
      "Trainable params: 151,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE model statement\n",
    "ae = Model(input_img,z_decoded)\n",
    "ae.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the weights of a specific layer of the encoder model, before training are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity of layers in the model:  13\n",
      "The weigths in the conv2d is:  [<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 64) dtype=float32, numpy=\n",
      "array([[[[ 0.04087575,  0.05895998, -0.00430713, ..., -0.08326497,\n",
      "          -0.04953241, -0.03582999],\n",
      "         [ 0.07479501, -0.00348064,  0.07686475, ...,  0.07988627,\n",
      "           0.00465287,  0.06445852],\n",
      "         [-0.04686734,  0.03561288, -0.02388179, ...,  0.01475448,\n",
      "          -0.05125507, -0.00532942]],\n",
      "\n",
      "        [[ 0.01826275, -0.00926737, -0.04323723, ..., -0.05338909,\n",
      "          -0.05304727, -0.02992889],\n",
      "         [ 0.01406843, -0.0106613 ,  0.08592266, ...,  0.08037448,\n",
      "          -0.02010875,  0.07103062],\n",
      "         [ 0.01101224,  0.0943874 ,  0.07306336, ..., -0.0041161 ,\n",
      "          -0.08485003, -0.00857042]],\n",
      "\n",
      "        [[-0.02545605,  0.07075962, -0.00906321, ...,  0.01940541,\n",
      "          -0.03709448,  0.01455449],\n",
      "         [ 0.07568547,  0.00201162, -0.05924144, ..., -0.03909023,\n",
      "           0.02208322,  0.06256922],\n",
      "         [-0.03270899,  0.06547144,  0.02607465, ..., -0.04508356,\n",
      "           0.09773083, -0.05313126]]],\n",
      "\n",
      "\n",
      "       [[[ 0.0603776 , -0.00898704,  0.05888617, ...,  0.01659153,\n",
      "           0.03954175, -0.00229782],\n",
      "         [ 0.07076024, -0.05538758,  0.01974309, ..., -0.07287765,\n",
      "           0.06831357, -0.02522134],\n",
      "         [-0.01065759, -0.06721575,  0.07117562, ..., -0.06690536,\n",
      "          -0.07018133, -0.08223818]],\n",
      "\n",
      "        [[ 0.04238912,  0.07800427, -0.05964795, ...,  0.07594991,\n",
      "          -0.01564257,  0.08120972],\n",
      "         [-0.09448017,  0.00945841,  0.00097853, ..., -0.02841356,\n",
      "           0.08316101,  0.05762951],\n",
      "         [-0.02823362, -0.09875809,  0.0555357 , ...,  0.04404627,\n",
      "           0.05049221, -0.03937479]],\n",
      "\n",
      "        [[ 0.07809913, -0.06678393,  0.01922528, ..., -0.00308958,\n",
      "          -0.04454648,  0.03141208],\n",
      "         [-0.08478579,  0.07570072, -0.08886959, ..., -0.03815356,\n",
      "          -0.0234469 , -0.00539222],\n",
      "         [ 0.00018425, -0.01912023, -0.03045308, ..., -0.08552913,\n",
      "          -0.08015591, -0.05994259]]],\n",
      "\n",
      "\n",
      "       [[[ 0.03066231,  0.04300804,  0.03523773, ...,  0.09794644,\n",
      "          -0.01891475, -0.08129866],\n",
      "         [ 0.05850713, -0.00949689, -0.0577512 , ...,  0.00264104,\n",
      "          -0.04616436,  0.02291972],\n",
      "         [-0.05926429, -0.04514387, -0.00065129, ...,  0.01496862,\n",
      "          -0.01586864,  0.03389232]],\n",
      "\n",
      "        [[ 0.08367543,  0.0609715 , -0.06572185, ..., -0.07832202,\n",
      "           0.05687536, -0.02417422],\n",
      "         [-0.04975527, -0.09017825,  0.09775147, ..., -0.02014928,\n",
      "          -0.01075945, -0.0611065 ],\n",
      "         [ 0.01458208, -0.00449353, -0.04522599, ..., -0.09052145,\n",
      "           0.05786264,  0.00925931]],\n",
      "\n",
      "        [[ 0.07511857,  0.06512758,  0.00723468, ..., -0.00536383,\n",
      "          -0.02518619, -0.06055356],\n",
      "         [ 0.03017622,  0.05490494, -0.06646127, ...,  0.05469202,\n",
      "           0.08441079, -0.05402042],\n",
      "         [ 0.0641394 , -0.01432116,  0.04813053, ..., -0.03455865,\n",
      "          -0.05823922, -0.07494177]]]], dtype=float32)>, <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "layer_number = 1\n",
    "print(\"quantity of layers in the model: \", len(encoder.layers))\n",
    "print(\"The weigths in the\", encoder.layers[layer_number].name, \"is: \", encoder.layers[layer_number].weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the weights of a specific layer of the decoder model,before training are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_number = 1\n",
    "# print(\"quantity of layers in the model: \", len(decoder.layers))\n",
    "# print(\"The weigths in the\", decoder.layers[layer_number].name, \"is: \", decoder.layers[layer_number].weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the weights of a specific layer of the autoencoder model,before training are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_number = 1\n",
    "# print(\"quantity of layers in the model: \", len(ae.layers))\n",
    "# print(\"The weigths in the\", ae.layers[layer_number].name, \"is: \", ae.layers[layer_number].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 7\n  y sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 7\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from tensorflow.keras.callbacks import EarlyStopping\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# run the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=5, mode='auto')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# history = ae.fit(train_generator, epochs=400, validation_data=validation_generator, verbose=1, shuffle = True)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m history \u001b[39m=\u001b[39m ae\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mlist_batches_training,y\u001b[39m=\u001b[39;49mlist_batches_training, epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mlist_batches_val_pos, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, shuffle \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:66\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_method_wrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     68\u001b[0m   \u001b[39m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m   \u001b[39mif\u001b[39;00m dc_context\u001b[39m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:802\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    794\u001b[0m   (x, y, sample_weight), validation_data \u001b[39m=\u001b[39m (\n\u001b[1;32m    795\u001b[0m       data_adapter\u001b[39m.\u001b[39mtrain_validation_split((x, y, sample_weight),\n\u001b[1;32m    796\u001b[0m                                           validation_split\u001b[39m=\u001b[39mvalidation_split,\n\u001b[1;32m    797\u001b[0m                                           shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m    799\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[1;32m    800\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[1;32m    801\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m--> 802\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mDataHandler(\n\u001b[1;32m    803\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    804\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    805\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    806\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    807\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    808\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    809\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    810\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    811\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    812\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    813\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    814\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    815\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    817\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m    818\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1101\u001b[0m     x,\n\u001b[1;32m   1102\u001b[0m     y,\n\u001b[1;32m   1103\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1104\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1105\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1106\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1107\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1108\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1109\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1110\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1111\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mds_context\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1112\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m   1114\u001b[0m strategy \u001b[39m=\u001b[39m ds_context\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1115\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter\u001b[39m.\u001b[39mget_dataset()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:282\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    280\u001b[0m         label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(data)))\n\u001b[1;32m    281\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPlease provide data which shares the same first dimension.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 282\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    283\u001b[0m num_samples \u001b[39m=\u001b[39m num_samples\u001b[39m.\u001b[39mpop()\n\u001b[1;32m    285\u001b[0m \u001b[39m# If batch_size is not passed but steps is, calculate from the input data.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# Default to 32 for backwards compat.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 7\n  y sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 7\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# run the model\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=5, mode='auto')\n",
    "# history = ae.fit(train_generator, epochs=400, validation_data=validation_generator, verbose=1, shuffle = True)\n",
    "\n",
    "history = ae.fit(x=list_batches_training,y=list_batches_training, epochs=300, validation_data=list_batches_val_pos, verbose=1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error matching the samples of the training and the validation sets, or something related."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
